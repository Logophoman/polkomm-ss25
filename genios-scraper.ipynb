{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff636a9",
   "metadata": {},
   "source": [
    "# Genios.de Artikel-Scraper (Deutsch - Rev. 2)\n",
    "\n",
    "Dieses Skript extrahiert Artikel-Metadaten von Genios.de für einen bestimmten Suchbegriff und Zeitraum.\n",
    "Aufgrund einer Begrenzung von 10.000 Ergebnissen pro Abfrage iteriert es Tag für Tag, um alle Ergebnisse zu sammeln.\n",
    "\n",
    "**Haftungsausschluss:**\n",
    "Web-Scraping sollte verantwortungsbewusst und ethisch korrekt durchgeführt werden.\n",
    "- Überprüfen Sie immer die `robots.txt` der Webseite (z.B. `https://www.genios.de/robots.txt`) und deren Nutzungsbedingungen.\n",
    "- Überlasten Sie den Server nicht; implementieren Sie Verzögerungen zwischen den Anfragen.\n",
    "- Dieses Skript dient Bildungszwecken und setzt die Einhaltung geltender Gesetze voraus, wie z.B. § 60d UrhG (Text und Data Mining für wissenschaftliche Forschung), falls anwendbar.\n",
    "\n",
    "Das Skript extrahiert:\n",
    "- Artikel-ID (Attribut)\n",
    "- Dokumenten-ID (Attribut)\n",
    "- Titel\n",
    "- Snippet (Teaser-Text)\n",
    "- Trefferumgebung / Kontext-Snippets (Kontext des Suchbegriffs, robuster extrahiert)\n",
    "- Quellenname (z.B. Frankfurter Allgemeine Sonntagszeitung (FAS))\n",
    "- Quellen-Kürzel (z.B. FAS)\n",
    "- Artikeldatum\n",
    "- Artikel-URL (Link zur Genios-Dokumentenseite)\n",
    "- Wortanzahl\n",
    "- Preis (falls verfügbar)\n",
    "- Vorschaubild-URL\n",
    "- Abfragedatum (das Datum, für das die Suche durchgeführt wurde)\n",
    "\n",
    "und speichert alles in einer CSV-Datei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670cabab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta, date\n",
    "import csv\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5873206",
   "metadata": {},
   "source": [
    "## Konfiguration\n",
    "Stellen Sie hier die Parameter für das Scraping ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e6514",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUCHBEGRIFF = \"AFD\"\n",
    "# Datumsbereich (einschließlich)\n",
    "START_DATUM_STR = \"01.09.2024\"\n",
    "ENDE_DATUM_STR = \"01.09.2024\" # Für Tests ggf. zuerst einen kleineren Bereich wählen, z.B. nur einen Tag\n",
    "\n",
    "# Basis-URL für Suchergebnisse\n",
    "BASIS_URL = \"https://www.genios.de/searchResult/Alle/Presse\"\n",
    "\n",
    "# Basis-Parameter für die Suche\n",
    "BASIS_PARAMETER = {\n",
    "    'requestText': SUCHBEGRIFF,\n",
    "    'category': 'Presse Deutschland',\n",
    "    'size': 100,  # Anzahl Ergebnisse pro Seite\n",
    "    'sort': 'BY_DATE',\n",
    "    'order': 'desc',\n",
    "    'resultListType': 'DEFAULT',\n",
    "    'view': 'list'\n",
    "}\n",
    "\n",
    "# Name der Ausgabe-CSV-Datei\n",
    "zeitstempel = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "AUSGABE_CSV_DATEI = f\"genios_artikel_{SUCHBEGRIFF.lower()}_{zeitstempel}.csv\"\n",
    "\n",
    "# HTTP-Header, um einen Browser zu simulieren\n",
    "HTTP_HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Verzögerung zwischen Anfragen (in Sekunden)\n",
    "MIN_VERZOEGERUNG = 1.5\n",
    "MAX_VERZOEGERUNG = 3.5\n",
    "\n",
    "# CSV Spaltennamen (wichtig für konsistente Reihenfolge)\n",
    "CSV_SPALTENNAMEN = [\n",
    "    'article_id_attr', 'document_id_attr', 'checkbox_value_id',\n",
    "    'title', 'snippet', 'context_snippet',\n",
    "    'source_name_full', 'source_name_short', 'source_icon_url',\n",
    "    'date_published', 'article_url', 'word_count',\n",
    "    'price', 'preview_image_url', 'query_date' # Das Datum, für das die Abfrage gemacht wurde\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51245a",
   "metadata": {},
   "source": [
    "## Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5b4d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generiere_datumsbereich(start_str, ende_str):\n",
    "    \"\"\"Generiert eine Liste von Daten (dd.mm.yyyy) zwischen Start- und Enddatum (einschließlich).\"\"\"\n",
    "    start_dt = datetime.strptime(start_str, \"%d.%m.%Y\")\n",
    "    ende_dt = datetime.strptime(ende_str, \"%d.%m.%Y\")\n",
    "    delta = timedelta(days=1)\n",
    "    daten_liste = []\n",
    "    aktuelles_datum_dt = start_dt\n",
    "    while aktuelles_datum_dt <= ende_dt:\n",
    "        daten_liste.append(aktuelles_datum_dt.strftime(\"%d.%m.%Y\"))\n",
    "        aktuelles_datum_dt += delta\n",
    "    return daten_liste\n",
    "\n",
    "def hole_text_sicher(element, standard=\"\"):\n",
    "    \"\"\"Extrahiert sicher Text von einem BeautifulSoup-Element.\"\"\"\n",
    "    return element.get_text(strip=True) if element else standard\n",
    "\n",
    "def hole_attribut_sicher(element, attribut, standard=\"\"):\n",
    "    \"\"\"Extrahiert sicher ein Attribut von einem BeautifulSoup-Element.\"\"\"\n",
    "    return element[attribut] if element and element.has_attr(attribut) else standard\n",
    "\n",
    "def get_zuletzt_gescrapetes_datum(csv_pfad, datums_spalte='query_date', datums_format='%d.%m.%Y'):\n",
    "    \"\"\"Liest die CSV-Datei und gibt das letzte verarbeitete 'query_date' zurück oder None.\"\"\"\n",
    "    if not os.path.exists(csv_pfad):\n",
    "        return None\n",
    "    \n",
    "    letztes_datum = None\n",
    "    try:\n",
    "        with open(csv_pfad, 'r', newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            if datums_spalte not in reader.fieldnames:\n",
    "                print(f\"Warnung: Spalte '{datums_spalte}' nicht in CSV gefunden für Fortsetzungslogik.\")\n",
    "                return None\n",
    "            \n",
    "            gefundene_daten = []\n",
    "            for zeile in reader:\n",
    "                datum_str = zeile.get(datums_spalte)\n",
    "                if datum_str:\n",
    "                    try:\n",
    "                        gefundene_daten.append(datetime.strptime(datum_str, datums_format).date())\n",
    "                    except ValueError:\n",
    "                        print(f\"Warnung: Konnte Datum '{datum_str}' nicht parsen in {csv_pfad}\")\n",
    "            if gefundene_daten:\n",
    "                letztes_datum = max(gefundene_daten)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Lesen der CSV für Fortsetzungslogik: {e}\")\n",
    "        return None \n",
    "    \n",
    "    return letztes_datum.strftime(datums_format) if letztes_datum else None\n",
    "\n",
    "def bereinige_snippet_text(text):\n",
    "    \"\"\"Bereinigt extrahierten Snippet-Text.\"\"\"\n",
    "    if not text: return \"\"\n",
    "    # Entferne führende/abschließende '..' und überflüssige Leerzeichen\n",
    "    text = re.sub(r\"^\\.\\.(.*?)\\.\\.$\", r\"\\1\", text.strip()) # Für ..Text..\n",
    "    text = re.sub(r\"^\\.\\.\", \"\", text.strip()) # Für ..Text\n",
    "    text = re.sub(r\"\\.\\.$\", \"\", text.strip()) # Für Text..\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Normalisiere Leerzeichen\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca3dc07",
   "metadata": {},
   "source": [
    "## Funktionen für das Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d77784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrahiere_artikel_daten(artikel_html_element, basis_url_fuer_joins, abgefragtes_datum_str):\n",
    "    \"\"\"Extrahiert Daten aus einem einzelnen <article>-HTML-Element.\"\"\"\n",
    "    daten = {'query_date': abgefragtes_datum_str}\n",
    "\n",
    "    daten['article_id_attr'] = hole_attribut_sicher(artikel_html_element, 'id')\n",
    "    daten['document_id_attr'] = hole_attribut_sicher(artikel_html_element, 'data-document-id')\n",
    "    \n",
    "    checkbox = artikel_html_element.find('input', class_='article__checkbox')\n",
    "    daten['checkbox_value_id'] = hole_attribut_sicher(checkbox, 'value')\n",
    "    daten['source_name_short'] = hole_attribut_sicher(checkbox, 'data-source')\n",
    "    \n",
    "    img_tag_main_div = artikel_html_element.find('div', class_='article__img')\n",
    "    if img_tag_main_div:\n",
    "        img_tag_main = img_tag_main_div.find('img', class_='preview-image')\n",
    "        preview_img_src = hole_attribut_sicher(img_tag_main, 'src')\n",
    "        daten['preview_image_url'] = urljoin(basis_url_fuer_joins, preview_img_src) if preview_img_src else \"\"\n",
    "    else:\n",
    "        daten['preview_image_url'] = \"\"\n",
    "    \n",
    "    text_bereich = artikel_html_element.find('div', class_='article__text')\n",
    "    if text_bereich:\n",
    "        titel_div = text_bereich.find('div', class_='article__text__title')\n",
    "        daten['title'] = hole_text_sicher(titel_div)\n",
    "\n",
    "        panel_body_details = text_bereich.find('a', class_='article__text__panelBody__details')\n",
    "        if panel_body_details:\n",
    "            # Snippet (Teaser)\n",
    "            snippet_div = panel_body_details.find('div', class_='article__text__text')\n",
    "            daten['snippet'] = bereinige_snippet_text(hole_text_sicher(snippet_div))\n",
    "\n",
    "            # Kontext-Snippet (Trefferumgebung) - Robuster Ansatz\n",
    "            gefundene_kontexte = []\n",
    "            # 1. Spezifisches Div suchen\n",
    "            kontext_div_spezifisch = panel_body_details.find('div', class_='article__text__trefferumgebung')\n",
    "            if kontext_div_spezifisch:\n",
    "                gefundene_kontexte.append(bereinige_snippet_text(hole_text_sicher(kontext_div_spezifisch)))\n",
    "            \n",
    "            # 2. Generell nach <mark>-Tags im gesamten panelBody suchen, falls spezifisches Div nicht ausreicht oder fehlt\n",
    "            #    Dies kann zu Duplikaten führen, wenn 'trefferumgebung' bereits <mark> enthält, daher nachher Deduplizierung.\n",
    "            for elem_mit_mark in panel_body_details.find_all(lambda tag: tag.name == 'div' and tag.find('mark', class_='highlight')):\n",
    "                # Wir wollen nicht das 'article__text__title' oder 'article__text__text' erneut als Kontext, \n",
    "                # wenn es nur um die Trefferumgebung geht.\n",
    "                if 'article__text__title' not in elem_mit_mark.get('class', []) and \\\n",
    "                   'article__text__text' not in elem_mit_mark.get('class', []):\n",
    "                    text_content = bereinige_snippet_text(elem_mit_mark.get_text(separator=' ', strip=True))\n",
    "                    if text_content and text_content not in gefundene_kontexte: # Nur hinzufügen, wenn nicht schon vom spezifischen Div erfasst\n",
    "                         gefundene_kontexte.append(text_content)\n",
    "            \n",
    "            # Wenn keine spezifische Trefferumgebung da war, aber der normale Snippet Highlights hat, diesen auch nehmen\n",
    "            if not kontext_div_spezifisch and snippet_div and snippet_div.find('mark', class_='highlight'):\n",
    "                highlight_text = bereinige_snippet_text(hole_text_sicher(snippet_div))\n",
    "                if highlight_text not in gefundene_kontexte:\n",
    "                     gefundene_kontexte.append(highlight_text)\n",
    "\n",
    "            daten['context_snippet'] = \" ..//.. \".join(filter(None, gefundene_kontexte)) # Mit Trenner verbinden, leere Strings filtern\n",
    "        else:\n",
    "            daten['snippet'] = \"\"\n",
    "            daten['context_snippet'] = \"\"\n",
    "\n",
    "        link_tag_header = text_bereich.find('a', class_='article__text__panelHeader')\n",
    "        artikel_href = hole_attribut_sicher(link_tag_header, 'href')\n",
    "        daten['article_url'] = urljoin(basis_url_fuer_joins, artikel_href) if artikel_href else \"\"\n",
    "\n",
    "        if link_tag_header:\n",
    "            quelle_mit_icon_div = link_tag_header.find('div', class_='article__img__source')\n",
    "            if quelle_mit_icon_div and quelle_mit_icon_div.find('span'):\n",
    "                daten['source_name_full'] = hole_text_sicher(quelle_mit_icon_div.find('span'))\n",
    "                quelle_icon_tag = quelle_mit_icon_div.find('img', class_='article__text__source-icon')\n",
    "                icon_src = hole_attribut_sicher(quelle_icon_tag, 'src')\n",
    "                daten['source_icon_url'] = urljoin(basis_url_fuer_joins, icon_src) if icon_src else \"\"\n",
    "            else:\n",
    "                quelle_nur_text_div = link_tag_header.find('div', class_='article__text__source')\n",
    "                if quelle_nur_text_div:\n",
    "                    voller_text = hole_text_sicher(quelle_nur_text_div)\n",
    "                    daten['source_name_full'] = voller_text.split('/')[0].strip() if '/' in voller_text else voller_text\n",
    "                else:\n",
    "                    daten['source_name_full'] = daten.get('source_name_short', '')\n",
    "                daten['source_icon_url'] = \"\" # Kein Icon in diesem Pfad erwartet\n",
    "        else:\n",
    "            daten['source_name_full'] = daten.get('source_name_short', '')\n",
    "            daten['source_icon_url'] = \"\"\n",
    "\n",
    "        text_bar_div = text_bereich.find('div', class_='article__text__bar')\n",
    "        if text_bar_div:\n",
    "            wortanzahl_div = text_bar_div.find('div', class_='article__text__bar__number_pages')\n",
    "            daten['word_count'] = hole_text_sicher(wortanzahl_div).replace('\\xa0', ' ').strip()\n",
    "            kauf_button_container = text_bar_div.find('div', class_='tooltip-button')\n",
    "            if kauf_button_container:\n",
    "                 kauf_button = kauf_button_container.find('a', class_='buy_button')\n",
    "                 daten['price'] = hole_attribut_sicher(kauf_button, 'data-price')\n",
    "            else:\n",
    "                daten['price'] = \"\"\n",
    "        else:\n",
    "            daten['word_count'] = \"\"\n",
    "            daten['price'] = \"\"\n",
    "    else:\n",
    "        for key in ['title', 'snippet', 'context_snippet', 'article_url', 'source_name_full', 'source_icon_url', 'word_count', 'price']:\n",
    "            daten[key] = \"\"\n",
    "\n",
    "    artikel_quelle_div = artikel_html_element.find('div', class_='article__source')\n",
    "    if artikel_quelle_div:\n",
    "        datum_div = artikel_quelle_div.find('div', class_='article__source__date')\n",
    "        daten['date_published'] = hole_text_sicher(datum_div)\n",
    "    else:\n",
    "        daten['date_published'] = \"\"\n",
    "\n",
    "    bereinigte_daten = {}\n",
    "    for spalte in CSV_SPALTENNAMEN:\n",
    "        bereinigte_daten[spalte] = daten.get(spalte, \"\")\n",
    "    return bereinigte_daten\n",
    "\n",
    "def scrape_tag_daten(ziel_datum_str, csv_schreiber, http_sitzung, api_basis_url, tages_parameter, min_v, max_v):\n",
    "    \"\"\"Verarbeitet das Scraping für einen einzelnen Tag, inklusive Paginierung.\"\"\"\n",
    "    print(f\"\\n--- Scrape für Datum: {ziel_datum_str} ---\")\n",
    "    aktueller_offset = 0\n",
    "    artikel_heute_gefunden = 0\n",
    "\n",
    "    while True:\n",
    "        parameter = tages_parameter.copy()\n",
    "        parameter['date'] = [f\"from_{ziel_datum_str}\", f\"to_{ziel_datum_str}\"]\n",
    "        parameter['offset'] = aktueller_offset\n",
    "\n",
    "        try:\n",
    "            print(f\"Rufe ab: {api_basis_url} mit Offset {aktueller_offset} für Datum {ziel_datum_str}\")\n",
    "            # Header werden von der Session genommen\n",
    "            antwort = http_sitzung.get(api_basis_url, params=parameter, timeout=30) \n",
    "            antwort.raise_for_status()\n",
    "            time.sleep(random.uniform(min_v, max_v))\n",
    "\n",
    "            soup = BeautifulSoup(antwort.content, 'html.parser')\n",
    "            artikel_auf_seite = soup.find_all('article', class_='article element')\n",
    "\n",
    "            if not artikel_auf_seite:\n",
    "                if aktueller_offset == 0:\n",
    "                    print(f\"Keine Artikel für {ziel_datum_str} gefunden.\")\n",
    "                else:\n",
    "                    print(f\"Keine weiteren Artikel auf Folgeseiten für {ziel_datum_str}. Wechsle zum nächsten Datum.\")\n",
    "                break\n",
    "\n",
    "            print(f\"{len(artikel_auf_seite)} Artikel auf dieser Seite gefunden (Offset {aktueller_offset}). Parse Daten...\")\n",
    "            artikel_heute_gefunden += len(artikel_auf_seite)\n",
    "\n",
    "            for artikel_html in artikel_auf_seite:\n",
    "                extrahierte_daten = extrahiere_artikel_daten(artikel_html, api_basis_url, ziel_datum_str)\n",
    "                csv_schreiber.writerow(extrahierte_daten)\n",
    "            \n",
    "            aktueller_offset += parameter['size']\n",
    "            if aktueller_offset >= 10000:\n",
    "                print(f\"10.000 Ergebnisse-Limit für Datum {ziel_datum_str} erreicht. Wechsle zum nächsten Datum.\")\n",
    "                break\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"HTTP-Fehler für {ziel_datum_str} bei Offset {aktueller_offset}: {e}\")\n",
    "            print(\"Möglicherweise Ende der Ergebnisse mit Fehlerseite erreicht oder blockiert. Wechsle zum nächsten Datum.\")\n",
    "            break \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Anfrage fehlgeschlagen für {ziel_datum_str} bei Offset {aktueller_offset}: {e}\")\n",
    "            print(\"Versuch wird übersprungen. Für robustere Lösung Retries implementieren.\")\n",
    "            break \n",
    "        except Exception as e:\n",
    "            print(f\"Ein unerwarteter Fehler ist beim Verarbeiten von {ziel_datum_str} bei Offset {aktueller_offset} aufgetreten: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            break \n",
    "    \n",
    "    print(f\"Scraping für {ziel_datum_str} abgeschlossen. Artikel heute gefunden: {artikel_heute_gefunden}\")\n",
    "    return artikel_heute_gefunden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a70447",
   "metadata": {},
   "source": [
    "## Hauptausführung des Skripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c4bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starte Scraping für '{SUCHBEGRIFF}' von {START_DATUM_STR} bis {ENDE_DATUM_STR}\")\n",
    "print(f\"Ausgabe wird gespeichert in: {AUSGABE_CSV_DATEI}\")\n",
    "\n",
    "gesamter_datumsbereich = generiere_datumsbereich(START_DATUM_STR, ENDE_DATUM_STR)\n",
    "zu_verarbeitende_daten = []\n",
    "\n",
    "letztes_datum_aus_csv_str = get_zuletzt_gescrapetes_datum(AUSGABE_CSV_DATEI)\n",
    "\n",
    "if letztes_datum_aus_csv_str:\n",
    "    print(f\"Fortsetzung erkannt. Letztes verarbeitetes Datum in CSV: {letztes_datum_aus_csv_str}\")\n",
    "    letztes_datum_dt = datetime.strptime(letztes_datum_aus_csv_str, \"%d.%m.%Y\").date()\n",
    "    for datum_str_check in gesamter_datumsbereich:\n",
    "        aktuelles_pruefdatum_dt = datetime.strptime(datum_str_check, \"%d.%m.%Y\").date()\n",
    "        if aktuelles_pruefdatum_dt > letztes_datum_dt:\n",
    "            zu_verarbeitende_daten.append(datum_str_check)\n",
    "    if not zu_verarbeitende_daten and letztes_datum_dt >= datetime.strptime(ENDE_DATUM_STR, \"%d.%m.%Y\").date():\n",
    "         print(\"Alle Daten im angegebenen Bereich scheinen bereits verarbeitet worden zu sein.\")\n",
    "    elif not zu_verarbeitende_daten and letztes_datum_dt < datetime.strptime(ENDE_DATUM_STR, \"%d.%m.%Y\").date():\n",
    "        print(f\"Keine neueren Daten nach {letztes_datum_aus_csv_str} im definierten Bereich gefunden. Überprüfen Sie den Datumsbereich oder ob der letzte Tag unvollständig war (wird nicht automatisch nachgeholt).\")\n",
    "else:\n",
    "    print(\"Keine existierende CSV-Datei gefunden oder keine Daten darin. Starte von vorne.\")\n",
    "    zu_verarbeitende_daten = gesamter_datumsbereich\n",
    "\n",
    "if not zu_verarbeitende_daten and not (letztes_datum_aus_csv_str and datetime.strptime(letztes_datum_aus_csv_str, \"%d.%m.%Y\").date() >= datetime.strptime(ENDE_DATUM_STR, \"%d.%m.%Y\").date()) :\n",
    "    if letztes_datum_aus_csv_str is None: \n",
    "        print(\"Keine Daten zum Verarbeiten basierend auf dem initialen Datumsbereich.\")\n",
    "\n",
    "gesamtzahl_artikel_gesammelt = 0\n",
    "if zu_verarbeitende_daten:\n",
    "    print(f\"{len(zu_verarbeitende_daten)} Tage werden verarbeitet.\")\n",
    "    datei_existiert_bereits = os.path.exists(AUSGABE_CSV_DATEI) and os.path.getsize(AUSGABE_CSV_DATEI) > 0\n",
    "\n",
    "    with requests.Session() as sitzung:\n",
    "        sitzung.headers.update(HTTP_HEADERS)\n",
    "        with open(AUSGABE_CSV_DATEI, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "            schreiber = csv.DictWriter(csvfile, fieldnames=CSV_SPALTENNAMEN)\n",
    "            if not datei_existiert_bereits:\n",
    "                schreiber.writeheader()\n",
    "                print(\"CSV-Header geschrieben.\")\n",
    "            \n",
    "            for idx, datum_str in enumerate(zu_verarbeitende_daten):\n",
    "                print(f\"Fortschritt: Tag {idx+1} von {len(zu_verarbeitende_daten)}\")\n",
    "                artikel_an_diesem_tag = scrape_tag_daten(\n",
    "                    datum_str, \n",
    "                    schreiber, \n",
    "                    sitzung, \n",
    "                    BASIS_URL, \n",
    "                    BASIS_PARAMETER, \n",
    "                    MIN_VERZOEGERUNG, \n",
    "                    MAX_VERZOEGERUNG\n",
    "                )\n",
    "                # Gesamtzahl wird nicht mehr hier summiert, da bei Fortsetzung ungenau.\n",
    "                # Zählung am Ende aus der Datei ist genauer.\n",
    "                csvfile.flush()\n",
    "else:\n",
    "    if not (letztes_datum_aus_csv_str and datetime.strptime(letztes_datum_aus_csv_str, \"%d.%m.%Y\").date() >= datetime.strptime(ENDE_DATUM_STR, \"%d.%m.%Y\").date()):\n",
    "      print(\"Keine Tage zur Verarbeitung übrig oder initialer Datumsbereich leer.\")\n",
    "\n",
    "print(f\"\\n--- Scraping beendet ---\")\n",
    "if os.path.exists(AUSGABE_CSV_DATEI):\n",
    "    final_row_count = 0\n",
    "    try:\n",
    "        with open(AUSGABE_CSV_DATEI, 'r', newline='', encoding='utf-8') as f_count:\n",
    "            reader_count = csv.reader(f_count)\n",
    "            header = next(reader_count, None) # Header überspringen\n",
    "            if header:\n",
    "                 final_row_count = sum(1 for _ in reader_count)\n",
    "        print(f\"Insgesamt {final_row_count} Artikel-Datensätze in {AUSGABE_CSV_DATEI} gespeichert.\")\n",
    "        print(\"Hinweis: Diese Zahl kann Duplikate enthalten, falls Tage aufgrund von Abbrüchen erneut gescraped wurden.\")\n",
    "        print(\"Es wird empfohlen, die CSV-Datei nachträglich zu deduplizieren (z.B. mit Pandas).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Konnte die finale Zeilenzahl nicht ermitteln: {e}\")\n",
    "else:\n",
    "    print(\"Keine CSV-Datei erstellt, möglicherweise wurden keine Daten gefunden oder es gab Fehler.\")\n",
    "\n",
    "# Beispiel für nachträgliche Deduplizierung mit Pandas (auskommentiert):\n",
    "# import pandas as pd\n",
    "# try:\n",
    "#     df = pd.read_csv(AUSGABE_CSV_DATEI)\n",
    "#     print(f\"DataFrame vor Deduplizierung: {df.shape}\")\n",
    "#     # Annahme: 'document_id_attr' und 'query_date' sind gute Schlüssel für Eindeutigkeit\n",
    "#     # Wenn ein Artikel (gleiche ID) an einem Tag mehrmals vorkommt (durch teilweisen Rescrape), wird er entfernt.\n",
    "#     df_deduplicated = df.drop_duplicates(subset=['document_id_attr', 'query_date'], keep='first')\n",
    "#     print(f\"DataFrame nach Deduplizierung: {df_deduplicated.shape}\")\n",
    "#     # Optional: Bereinigte Daten speichern\n",
    "#     # df_deduplicated.to_csv(f\"deduplicated_{AUSGABE_CSV_DATEI}\", index=False, encoding='utf-8')\n",
    "#     # print(f\"Deduplizierte Daten gespeichert in: deduplicated_{AUSGABE_CSV_DATEI}\")\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"CSV-Datei {AUSGABE_CSV_DATEI} nicht gefunden für Pandas-Operation.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Fehler bei der Pandas-Verarbeitung: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bacc5d8",
   "metadata": {},
   "source": [
    "## Erläuterung des Codes\n",
    "\n",
    "1.  **Importe:** Notwendige Bibliotheken, inklusive `re` für reguläre Ausdrücke (Textbereinigung).\n",
    "2.  **Konfiguration:** Globale Variablen.\n",
    "3.  **Hilfsfunktionen:**\n",
    "    *   `generiere_datumsbereich`, `hole_text_sicher`, `hole_attribut_sicher`: Unverändert.\n",
    "    *   `get_zuletzt_gescrapetes_datum`: Unverändert; findet das letzte *Datum*, für das *alle* Paginierungsseiten erfolgreich abgerufen wurden (oder zumindest der Versuch dafür endete).\n",
    "    *   `bereinige_snippet_text`: Neue Funktion zur Säuberung von Text-Snippets (entfernt z.B. führende/folgende '..').\n",
    "4.  **Funktionen für das Scraping:**\n",
    "    *   `extrahiere_artikel_daten`:\n",
    "        *   **Context Snippet:** Extrahiert nun robuster. Es sucht zuerst nach `div.article__text__trefferumgebung`. Zusätzlich (oder alternativ) durchsucht es den `div.article__text__panelBody__details` nach allen `div`s, die `<mark>`-Tags enthalten (außer Titel und Haupt-Snippet selbst), um alle Kontexte zu sammeln. Die gesammelten Kontexte werden mit ` ..//.. ` verbunden.\n",
    "        *   **Quellen-Infos:** Die Logik zur Extraktion von `source_name_full` und `source_icon_url` aus dem `article__text__panelHeader` wurde verfeinert, um die verschiedenen möglichen Verschachtelungen (mit/ohne Icon-Div) besser zu handhaben.\n",
    "        *   Die Extraktion von `word_count` und `price` wurde in den `article__text__bar`-Block verschoben, da sie dort strukturell hingehören.\n",
    "        *   Verwendet `bereinige_snippet_text` für `snippet` und `context_snippet`.\n",
    "    *   `scrape_tag_daten`: Übergibt nun nicht mehr `standard_header` explizit, da die `http_sitzung` (Session) bereits die Header enthält.\n",
    "5.  **Hauptausführung des Skripts:**\n",
    "    *   Die Fortsetzungslogik ist im Kern gleich geblieben: Es werden Tage *nach* dem letzten erfolgreich in der CSV gefundenen `query_date` verarbeitet.\n",
    "    *   **Wichtig bzgl. Dopplungen:** Wenn ein Tag nicht vollständig gescraped wurde (z.B. bei Abbruch), wird dieser Tag beim nächsten Lauf *komplett neu* gescraped. Dies führt zu Duplikaten für die bereits erfassten Artikel dieses Tages. \n",
    "    *   Am Ende wird die Gesamtzahl der Zeilen in der CSV gezählt und ein expliziter Hinweis auf mögliche Duplikate und die Notwendigkeit der nachträglichen Deduplizierung gegeben.\n",
    "    *   Ein auskommentiertes Beispiel für die Deduplizierung mit Pandas wurde hinzugefügt.\n",
    "\n",
    "6.  **Benutzung im Jupyter Notebook (.ipynb):** Unverändert.\n",
    "\n",
    "7.  **Wichtige Überlegungen:**\n",
    "    *   **Deduplizierung:** Es ist **essenziell**, die resultierende CSV-Datei nachträglich zu deduplizieren, um saubere Daten für die Analyse zu erhalten. Das Pandas-Beispiel am Ende des Skripts zeigt einen Weg dafür.\n",
    "    *   **Robustheit der Kontext-Extraktion:** Die neue Methode ist deutlich robuster, aber bei sehr komplexen oder unerwarteten HTML-Änderungen könnten immer noch Anpassungen nötig sein."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
