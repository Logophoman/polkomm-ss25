{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc88fd0",
   "metadata": {},
   "source": [
    "# Sentiment-Analyse von Genios.de Artikel-Metadaten\n",
    "\n",
    "Dieses Notebook demonstriert, wie man mithilfe des vortrainierten multilingualen Sentiment-Analyse-Modells `tabularisai/multilingual-sentiment-analysis` von Hugging Face die Kombination aus Titel und Kontext-Snippets von Artikeln analysiert, die zuvor von Genios.de gescraped und in einer CSV-Datei gespeichert wurden.\n",
    "\n",
    "**Wichtig:** Dieses Notebook dient zu Demonstrationszwecken. Im Rahmen des Kurses werden auch die Limitationen dieser Technik erläutert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f01ddd",
   "metadata": {},
   "source": [
    "## 1. Vorbereitung und CSV-Datei Upload\n",
    "\n",
    "Stellen Sie sicher, dass Sie die zuvor generierte CSV-Datei (z.B. `genios_artikel_afd_YYYYMMDD_HHMMSS.csv`) in das Arbeitsverzeichnis dieses Colab-Notebooks hochgeladen haben oder den Pfad korrekt angeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788795d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installiere die 'transformers' Bibliothek, falls noch nicht geschehen\n",
    "!pip install transformers[sentencepiece] pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1091ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiere benötigte Bibliotheken\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "print('Bibliotheken importiert.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f190fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Laden der CSV-Daten\n",
    "# BITTE DEN DATEINAMEN ANPASSEN, falls Ihre Datei anders heißt!\n",
    "csv_filename = 'genios_afd_articles_20240901_120000.csv' # Beispielname, anpassen!\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_filename)\n",
    "    print(f'CSV-Datei \"{csv_filename}\" erfolgreich geladen.')\n",
    "    print(f'Anzahl der Artikel-Einträge: {len(df)}')\n",
    "    print(f'Spalten in der CSV: {df.columns.tolist()}')\n",
    "    # Zeige die ersten paar Zeilen zur Überprüfung\n",
    "    print(\"\\nErste paar Zeilen der Daten:\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"FEHLER: Die Datei '{csv_filename}' wurde nicht gefunden. Bitte laden Sie sie hoch oder passen Sie den Pfad an.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ein Fehler ist beim Laden der CSV-Datei aufgetreten: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e2ddb3",
   "metadata": {},
   "source": [
    "## 2. Deduplizierung (Wichtig!)\n",
    "\n",
    "Wie im Scraping-Notebook erwähnt, können durch die Fortsetzungslogik Duplikate entstehen. Wir sollten diese vor der Analyse entfernen, um die Ergebnisse nicht zu verfälschen und Rechenzeit zu sparen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6879eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals(): # Prüfen, ob DataFrame geladen wurde\n",
    "    print(f\"Datensätze vor Deduplizierung: {len(df)}\")\n",
    "    # Deduplizierung basierend auf einer eindeutigen Artikel-ID und dem Abfragedatum\n",
    "    # 'document_id_attr' ist wahrscheinlich ein guter Schlüssel.\n",
    "    # 'query_date' ist wichtig, falls derselbe Artikel an verschiedenen Tagen mit unterschiedlichen Kontexten auftaucht (unwahrscheinlich bei kurzen Zeiträumen).\n",
    "    if 'document_id_attr' in df.columns and 'query_date' in df.columns:\n",
    "        df.drop_duplicates(subset=['document_id_attr', 'query_date'], keep='first', inplace=True)\n",
    "        print(f\"Datensätze nach Deduplizierung: {len(df)}\")\n",
    "    else:\n",
    "        print(\"Warnung: Spalten 'document_id_attr' oder 'query_date' nicht für Deduplizierung gefunden. Überspringe Deduplizierung.\")\n",
    "else:\n",
    "    print(\"DataFrame 'df' nicht geladen. Deduplizierung kann nicht durchgeführt werden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c09336",
   "metadata": {},
   "source": [
    "## 3. Initialisierung der Sentiment-Analyse Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2befb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisiere die Sentiment-Analyse Pipeline\n",
    "# 'tabularisai/multilingual-sentiment-analysis' ist ein gutes mehrsprachiges Modell\n",
    "print('Initialisiere Sentiment-Analyse Pipeline... Dies kann einen Moment dauern, da das Modell heruntergeladen wird.')\n",
    "try:\n",
    "    sentiment_pipe = pipeline(\n",
    "        'text-classification', \n",
    "        model='tabularisai/multilingual-sentiment-analysis', \n",
    "        # device=0 # Uncomment für GPU-Nutzung, falls verfügbar und CUDA eingerichtet ist\n",
    "    )\n",
    "    print('Sentiment-Analyse Pipeline erfolgreich initialisiert.')\n",
    "except Exception as e:\n",
    "    print(f\"Fehler bei der Initialisierung der Pipeline: {e}\")\n",
    "    print(\"Stellen Sie sicher, dass Sie eine Internetverbindung haben und die Transformers-Bibliothek korrekt installiert ist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03724fa7",
   "metadata": {},
   "source": [
    "## 4. Analysefunktion und Durchführung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a99ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_for_text(text_to_analyze):\n",
    "    \"\"\"Analysiert den Sentiment eines Textes und gibt das Ergebnis zurück.\"\"\"\n",
    "    if not text_to_analyze or pd.isna(text_to_analyze): # Prüft auf leere Strings oder NaN\n",
    "        return {'label': 'NO_TEXT', 'score': 0.0}\n",
    "    try:\n",
    "        # Das Modell kann lange Texte verarbeiten, aber sehr lange Texte können zu \n",
    "        # Out-of-Memory-Fehlern führen oder die maximale Sequenzlänge überschreiten.\n",
    "        # Wir kürzen den Text ggf. auf eine vernünftige Länge (z.B. die ersten 512 Tokens des Modells).\n",
    "        # Die Pipeline sollte das intern handhaben, aber zur Sicherheit:\n",
    "        max_length = 500 # Zeichen, nicht Tokens, für eine einfache Kürzung\n",
    "        truncated_text = text_to_analyze[:max_length]\n",
    "        \n",
    "        result = sentiment_pipe(truncated_text)\n",
    "        if result and isinstance(result, list):\n",
    "            return result[0] # Das Ergebnis ist eine Liste mit einem Dictionary\n",
    "        else:\n",
    "            return {'label': 'Error_Parsing_Result', 'score': 0.0}\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Analyse von Text: '{str(text_to_analyze)[:50]}...': {e}\")\n",
    "        return {'label': 'Error_Analysis', 'score': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f44e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals() and 'sentiment_pipe' in locals():\n",
    "    print(\"Führe Sentiment-Analyse für Titel und Kontext-Snippets durch...\")\n",
    "    \n",
    "    # Listen für die Ergebnisse\n",
    "    sentiment_labels = []\n",
    "    sentiment_scores = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Iteriere über die Zeilen des DataFrames\n",
    "    # Begrenze für Demo-Zwecke die Anzahl der zu analysierenden Artikel, um Zeit zu sparen\n",
    "    # Entferne oder erhöhe .head(X), um alle Artikel zu analysieren\n",
    "    # for index, row in df.head(20).iterrows(): \n",
    "    for index, row in df.iterrows(): \n",
    "        title = str(row.get('title', ''))\n",
    "        context = str(row.get('context_snippet', ''))\n",
    "        \n",
    "        # Kombiniere Titel und Kontext für eine umfassendere Analyse\n",
    "        # Füge ein Trennzeichen hinzu, falls beide vorhanden sind\n",
    "        combined_text = title\n",
    "        if title and context:\n",
    "            combined_text += \" [SEP] \" + context\n",
    "        elif context: # Nur Kontext, kein Titel\n",
    "            combined_text = context\n",
    "        \n",
    "        if not combined_text.strip(): # Wenn nach Kombination immer noch leer\n",
    "            print(f\"Artikel {index+1} (ID: {row.get('document_id_attr', 'N/A')}): Kein Text für Analyse vorhanden.\")\n",
    "            sentiment_result = {'label': 'NO_TEXT', 'score': 0.0}\n",
    "        else:\n",
    "            sentiment_result = analyze_sentiment_for_text(combined_text)\n",
    "            print(f\"Artikel {index+1} (ID: {row.get('document_id_attr', 'N/A')}) - Sentiment: {sentiment_result['label']} (Score: {sentiment_result.get('score', 0.0):.4f})\")\n",
    "        \n",
    "        sentiment_labels.append(sentiment_result['label'])\n",
    "        sentiment_scores.append(sentiment_result.get('score', 0.0))\n",
    "        \n",
    "        # Statusausgabe alle X Artikel\n",
    "        if (index + 1) % 50 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"--- {index+1} Artikel verarbeitet in {elapsed_time:.2f} Sekunden ---\")\n",
    "\n",
    "    # Füge die Ergebnisse als neue Spalten zum DataFrame hinzu\n",
    "    df['sentiment_label'] = sentiment_labels\n",
    "    df['sentiment_score'] = sentiment_scores\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f'\\nSentiment-Analyse für {len(df)} Einträge abgeschlossen in {(end_time - start_time):.2f} Sekunden.')\n",
    "    \n",
    "    # Zeige die ersten Zeilen mit den neuen Sentiment-Spalten\n",
    "    print(\"\\nDataFrame mit Sentiment-Ergebnissen:\")\n",
    "    print(df[['title', 'context_snippet', 'sentiment_label', 'sentiment_score']].head())\n",
    "    \n",
    "    # Speichere den DataFrame mit den Sentiment-Ergebnissen (optional)\n",
    "    output_filename = f\"genios_artikel_mit_sentiment_{csv_filename.split('_')[-2]}_{csv_filename.split('_')[-1]}\"\n",
    "    df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "    # Alternativ als Excel, falls bevorzugt:\n",
    "    # df.to_excel(output_filename.replace('.csv', '.xlsx'), index=False)\n",
    "    print(f\"\\nDaten mit Sentiment-Analyse gespeichert in: {output_filename}\")\n",
    "\n",
    "else:\n",
    "    print(\"DataFrame 'df' oder 'sentiment_pipe' nicht initialisiert. Analyse kann nicht durchgeführt werden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1515d98e",
   "metadata": {},
   "source": [
    "## 5. Beispielhafte Auswertung\n",
    "\n",
    "Nachdem die Sentiment-Analyse durchgeführt wurde, können wir uns die Verteilung der Sentiments ansehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0fa104",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals() and 'sentiment_label' in df.columns:\n",
    "    print(\"\\nVerteilung der Sentiment-Labels:\")\n",
    "    sentiment_counts = df['sentiment_label'].value_counts()\n",
    "    print(sentiment_counts)\n",
    "    \n",
    "    # Einfache Visualisierung (optional, erfordert matplotlib)\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        sentiment_counts.plot(kind='bar', title='Verteilung der Sentiment-Labels')\n",
    "        plt.ylabel('Anzahl Artikel')\n",
    "        plt.xlabel('Sentiment Label')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        print(\"Matplotlib nicht installiert. Für eine Visualisierung bitte installieren: !pip install matplotlib\")\n",
    "else:\n",
    "    print(\"DataFrame oder Sentiment-Spalten nicht vorhanden für die Auswertung.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73b5da",
   "metadata": {},
   "source": [
    "## Fazit\n",
    "\n",
    "In diesem Notebook haben wir gezeigt, wie man:\n",
    "\n",
    "- Eine CSV-Datei mit gescrapten Artikel-Metadaten einliest und dedupliziert.\n",
    "- Die Kombination aus Artikeltitel und Kontext-Snippet für die Sentiment-Analyse vorbereitet.\n",
    "- Mithilfe eines vortrainierten, multilingualen Sentiment-Analysemodells von Hugging Face die Stimmung der kombinierten Texte klassifiziert.\n",
    "- Die Ergebnisse zurück in den DataFrame schreibt und optional speichert.\n",
    "- Eine einfache Auswertung der Sentiment-Verteilung durchführt.\n",
    "\n",
    "**Hinweis:** Dies ist ein Demonstrationscode. In einem echten Einsatzszenario sind weitere Vorverarbeitungsschritte, Fehlerbehandlungen und tiefgreifendere Analysen (z.B. Betrachtung der Scores, zeitliche Entwicklung etc.) ratsam. Die Qualität der Sentiment-Analyse hängt stark vom Modell und der Qualität/Länge der Eingabetexte ab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
