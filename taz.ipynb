{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping TAZ.de (Artikel & Kommentare) mit `requests` & `BeautifulSoup`\n",
    "\n",
    "Dieses Notebook demonstriert, wie man Artikeldetails und Kommentare von der Nachrichtenseite TAZ.de extrahieren kann, **ohne** auf Browser-Automatisierungstools wie Selenium angewiesen zu sein. Dies ist möglich, da TAZ.de viele seiner Inhalte, einschließlich der Kommentare, oft direkt im initialen HTML-Quelltext ausliefert (serverseitiges Rendering).\n",
    "\n",
    "**Ziele:**\n",
    "1.  Artikel-Metadaten (Titel, Kicker, Datum, etc.) extrahieren.\n",
    "2.  Artikel-Volltext extrahieren.\n",
    "3.  Kommentare, einschließlich verschachtelter Antworten, extrahieren und zählen.\n",
    "4.  Die Daten strukturiert speichern.\n",
    "\n",
    "**Verwendete Bibliotheken:**\n",
    "*   `requests`: Zum Abrufen des HTML-Codes der Webseite.\n",
    "*   `BeautifulSoup`: Zum Parsen des HTML-Codes und Extrahieren der Daten.\n",
    "*   `json`: Zum Speichern der Ergebnisse.\n",
    "*   `time`: Für Pausen (optional, aber empfohlen).\n",
    "\n",
    "**Wichtiger Hinweis:** Das Funktionieren dieses Skripts hängt von der aktuellen HTML-Struktur von TAZ.de ab. Webseiten ändern sich häufig, daher müssen die verwendeten Selektoren (z.B. `class_=` Namen) möglicherweise in Zukunft angepasst werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teil 0: Setup\n",
    "\n",
    "Installation (falls nötig) und Import der Bibliotheken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installieren der Bibliotheken (normalerweise in Colab vorinstalliert)\n",
    "# !pip install requests beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import random # Für optional zufällige Pausen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teil 1: Funktion zur Extraktion von Kommentaren (inkl. Subkommentare)\n",
    "\n",
    "Wir definieren zuerst eine Hilfsfunktion, die rekursiv die Anzahl der Antworten (Subkommentare) auf einen Kommentar zählt. Dies basiert auf der Struktur Ihres ursprünglichen Skripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recursively count subcomments (based on the provided taz-scraper code)\n",
    "def count_subcomments(element):\n",
    "    \"\"\"Zählt rekursiv die Anzahl der direkten und indirekten Subkommentare.\"\"\"\n",
    "    subcomments_count = 0\n",
    "    if element:\n",
    "        # Finde direkte Kind-Kommentare (li mit Klasse 'member')\n",
    "        subcomments = element.find_all('li', class_='member', recursive=False)\n",
    "        subcomments_count += len(subcomments)\n",
    "        # Für jeden direkten Subkommentar...\n",
    "        for subcomment in subcomments:\n",
    "            # ...finde dessen Container für weitere Subkommentare...\n",
    "            sub_subcomments_list = subcomment.find('ul', class_='thread')\n",
    "            # ...und rufe die Funktion rekursiv auf.\n",
    "            subcomments_count += count_subcomments(sub_subcomments_list)\n",
    "    return subcomments_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teil 2: Funktion zum Scrapen eines einzelnen TAZ-Artikels\n",
    "\n",
    "Diese Funktion nimmt eine URL entgegen, ruft die Seite ab, parst das HTML und extrahiert die gewünschten Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_taz_article(url, headers):\n",
    "    \"\"\"\n",
    "    Scrapt Details und Kommentare von einer gegebenen TAZ-Artikel-URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): Die URL des TAZ-Artikels.\n",
    "        headers (dict): HTTP-Header für die Anfrage (insb. User-Agent).\n",
    "\n",
    "    Returns:\n",
    "        dict: Ein Dictionary mit den gescrapten Daten oder None bei einem Fehler.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"  Versuche URL abzurufen: {url}\")\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status() # Fehler bei nicht-200 Status\n",
    "\n",
    "        print(f\"  Status Code: {response.status_code}\")\n",
    "        soup = BeautifulSoup(response.content, 'lxml') # 'lxml' ist meist schneller\n",
    "\n",
    "        # --- Artikelinformationen extrahieren ---\n",
    "        # Selektoren basieren auf dem bereitgestellten Skript - können sich ändern!\n",
    "        kicker_tag = soup.find('span', class_='kicker')\n",
    "        kicker = kicker_tag.get_text(strip=True) if kicker_tag else 'N/A'\n",
    "\n",
    "        # Titel kann in mehreren Spans sein, nimm den letzten\n",
    "        title = 'N/A'\n",
    "        title_h1 = soup.find('h1', itemprop='headline')\n",
    "        if title_h1:\n",
    "           title_spans = title_h1.find_all('span')\n",
    "           if title_spans:\n",
    "               title = title_spans[-1].get_text(strip=True)\n",
    "\n",
    "\n",
    "        summary_tag = soup.find('p', itemprop='description')\n",
    "        summary = summary_tag.get_text(strip=True) if summary_tag else 'N/A'\n",
    "\n",
    "        # Zeitstempel aus 'content'-Attribut extrahieren\n",
    "        time_tag = soup.find('li', itemprop='datePublished')\n",
    "        timestamp_iso = time_tag['content'] if time_tag and time_tag.has_attr('content') else 'N/A'\n",
    "\n",
    "\n",
    "        # Artikeltext - alle <p> mit Klasse 'article' zusammenfügen\n",
    "        article_p_tags = soup.find_all('p', class_='article')\n",
    "        article_content = ' '.join([p.get_text(strip=True) for p in article_p_tags])\n",
    "        if not article_content: # Fallback falls Klasse nicht (mehr) existiert\n",
    "            # Versuche einen allgemeineren Container zu finden (muss ggf. angepasst werden)\n",
    "            article_body = soup.find('div', class_='sectbody')\n",
    "            if article_body:\n",
    "                 article_p_tags = article_body.find_all('p')\n",
    "                 article_content = ' '.join([p.get_text(strip=True) for p in article_p_tags])\n",
    "            else:\n",
    "                 article_content = \"Artikeltext nicht gefunden (Selektor prüfen!)\"\n",
    "\n",
    "\n",
    "        # --- Kommentare extrahieren ---\n",
    "        comments_data = []\n",
    "        # Finde alle Top-Level Kommentare (li mit Klasse 'member', aber nicht in einem 'ul.thread')\n",
    "        top_level_comments = soup.select('ul.comments > li.member') # CSS-Selektor für Top-Level\n",
    "\n",
    "        print(f\"  Anzahl Top-Level Kommentare gefunden: {len(top_level_comments)}\")\n",
    "\n",
    "        for comment in top_level_comments:\n",
    "            name = 'N/A'\n",
    "            name_h4 = comment.find('h4')\n",
    "            if name_h4:\n",
    "                name = name_h4.get_text(strip=True)\n",
    "\n",
    "            profile_link = 'N/A'\n",
    "            author_link = comment.find('a', class_='author person')\n",
    "            if author_link and author_link.has_attr('href'):\n",
    "                profile_link = 'https://taz.de' + author_link['href']\n",
    "\n",
    "            comment_time = 'N/A'\n",
    "            time_tag_comment = comment.find('time')\n",
    "            if time_tag_comment and time_tag_comment.has_attr('datetime'):\n",
    "                comment_time = time_tag_comment['datetime']\n",
    "\n",
    "            # Kommentartext - kann in mehreren <p> sein\n",
    "            comment_text_parts = comment.find_all('p', class_=lambda x: x in x.split()) # Findet <p class=\"odd\">, <p class=\"even\"> etc.\n",
    "            if not comment_text_parts: # Fallback, falls Klassenstruktur anders ist\n",
    "                 comment_body_div = comment.find('div', class_='comment_text') # Beispiel für alternativen Selektor\n",
    "                 if comment_body_div:\n",
    "                     comment_text_parts = comment_body_div.find_all('p')\n",
    "\n",
    "            comment_text = ' '.join(p.get_text(strip=True) for p in comment_text_parts)\n",
    "\n",
    "            # Check for edits by 'taz' and remove the edit note from the comment text (optional)\n",
    "            # edited_by_taz = any('taz' in p.get_text() for p in comment_text_parts)\n",
    "            # if edited_by_taz:\n",
    "            #     comment_text = comment_text.replace('taz: \"Taz comment added in\"', '').strip() # Beispiel-Text anpassen\n",
    "\n",
    "            # Finde den Container für Subkommentare dieses Kommentars\n",
    "            subcomments_list_ul = comment.find('ul', class_='thread')\n",
    "            response_comments_count = count_subcomments(subcomments_list_ul)\n",
    "\n",
    "            comments_data.append({\n",
    "                'Name': name,\n",
    "                'Profile link': profile_link,\n",
    "                'Timestamp': comment_time,\n",
    "                'Comment text': comment_text,\n",
    "                'Response comments count': response_comments_count,\n",
    "            })\n",
    "\n",
    "        # --- Zusammenstellen der Artikeldaten ---\n",
    "        article_data = {\n",
    "            'URL': url,\n",
    "            'Kicker': kicker,\n",
    "            'Title': title,\n",
    "            'Summary': summary,\n",
    "            'Time': timestamp_iso,\n",
    "            'Article Text': article_content,\n",
    "            'Comments': comments_data,\n",
    "            'TopLevelCommentCount': len(top_level_comments) # Anzahl direkter Kommentare\n",
    "        }\n",
    "        print(f\"  Artikel erfolgreich verarbeitet.\")\n",
    "        return article_data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  Fehler beim Abrufen der URL {url}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"  Unerwarteter Fehler beim Verarbeiten von {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teil 3: Ausführung des Scrapings für Beispiel-URLs\n",
    "\n",
    "Wir definieren eine Liste von Beispiel-TAZ-URLs und wenden unsere Scraping-Funktion darauf an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel TAZ URLs (ersetzen oder erweitern Sie diese Liste)\n",
    "# Suchen Sie sich einige aktuelle TAZ-Artikel mit Kommentaren heraus.\n",
    "example_urls = [\n",
    "    \"https://taz.de/Gladbecker-Geiseldrama-vor-35-Jahren/!5949917/\", # Beispiel älter\n",
    "    \"https://taz.de/Debatte-ueber-Direktkandidaturen/!5997264/\",     # Beispiel neuer\n",
    "    \"https://taz.de/FDP-regiert-im-Bund-mit/!5997279/\",              # Beispiel mit anderer Struktur?\n",
    "    \"https://taz.de/!5997684/\"                                        # Beispiel ohne Kommentare?\n",
    "    # Fügen Sie hier weitere TAZ URLs hinzu\n",
    "]\n",
    "\n",
    "# Wichtig: User-Agent setzen!\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "all_scraped_data = []\n",
    "MAX_ARTICLES_DEMO = len(example_urls) # Scrapen aller Beispiel-URLs\n",
    "\n",
    "print(f\"Starte Scraping für {MAX_ARTICLES_DEMO} Beispiel-Artikel...\")\n",
    "\n",
    "for i, url in enumerate(example_urls):\n",
    "    print(f\"\\nVerarbeite Artikel {i+1}/{MAX_ARTICLES_DEMO}...\")\n",
    "    article_data = scrape_taz_article(url, headers)\n",
    "\n",
    "    if article_data:\n",
    "        all_scraped_data.append(article_data)\n",
    "\n",
    "    # Optionale Pause zwischen den Anfragen\n",
    "    pause = random.uniform(1.5, 3.5)\n",
    "    print(f\"  Pause für {pause:.1f} Sekunden...\")\n",
    "    time.sleep(pause)\n",
    "\n",
    "print(f\"\\nScraping abgeschlossen. {len(all_scraped_data)} Artikel erfolgreich verarbeitet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teil 4: Ergebnisse anzeigen und speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeige die Daten des ersten erfolgreich gescrapten Artikels (falls vorhanden)\n",
    "if all_scraped_data:\n",
    "    print(\"\\n--- Beispiel Daten (erster Artikel) ---\")\n",
    "    # Pretty print using json.dumps for better readability\n",
    "    print(json.dumps(all_scraped_data[0], indent=2, ensure_ascii=False))\n",
    "else:\n",
    "    print(\"\\nKeine Daten zum Anzeigen vorhanden.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ergebnisse in einer JSON-Datei speichern\n",
    "import datetime\n",
    "output_filename_taz = f\"taz_scraped_data_demo_{datetime.date.today()}.json\"\n",
    "\n",
    "try:\n",
    "    with open(output_filename_taz, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_scraped_data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"\\nErgebnisse wurden erfolgreich in '{output_filename_taz}' gespeichert.\")\n",
    "\n",
    "    # Optional: Datei im Colab herunterladen\n",
    "    # from google.colab import files\n",
    "    # files.download(output_filename_taz)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nFehler beim Speichern der Datei: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazit & Nächste Schritte\n",
    "\n",
    "Dieses Notebook hat gezeigt, dass es für bestimmte Webseiten wie TAZ.de möglich ist, auch komplexe Daten wie Kommentare nur mit `requests` und `BeautifulSoup` zu extrahieren.\n",
    "\n",
    "**Wichtige Punkte:**\n",
    "*   **Abhängigkeit von der Seitenstruktur:** Die verwendeten Selektoren (`find()`, `find_all()`, `select()`) sind spezifisch für die TAZ-Struktur zum Zeitpunkt der Erstellung. Änderungen an der Webseite können das Skript unbrauchbar machen. Regelmäßige Überprüfung und Anpassung sind notwendig.\n",
    "*   **Fehlerbehandlung:** Eine robuste Implementierung sollte mehr Fehlerbehandlung enthalten (z.B. wenn bestimmte Elemente nicht gefunden werden).\n",
    "*   **Ethik & Recht:** Auch hier gelten die Regeln für verantwortungsvolles Scraping (siehe `session2.html`): Nutzungsbedingungen prüfen, Server nicht überlasten (Pausen!), Datenschutz beachten.\n",
    "\n",
    "Sie können dieses Notebook als Grundlage verwenden und anpassen, um eine größere Anzahl von TAZ-Artikeln zu scrapen (z.B. indem Sie eine längere Liste von URLs bereitstellen)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}